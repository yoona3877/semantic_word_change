{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lsavectors.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "846Ya21aDv3U",
        "outputId": "f566627c-efe9-4acf-c702-d58b7d66dc64"
      },
      "source": [
        "#Step 1. Import NLTK in Python: http://www.nltk.org/. Download the Brown Corpus\n",
        "#http://www.nltk.org/book/ch02.html for analyses below\n",
        "\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "import csv \n",
        "from scipy import sparse, spatial, stats\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial import distance"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waft5iwqDz4m",
        "outputId": "284e1698-245f-4184-c68a-2dbd62eaed7f"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvSI8NR_Dx7q",
        "outputId": "ceccd642-3ec2-4270-9489-1ca59a740580"
      },
      "source": [
        "#Step 2. Extract the 5000 most common English words (denoted by W) based on unigram\n",
        "#frequencies in the Brown corpus. Report the 5 most and least common words you have found\n",
        "#in the 5000 words. Update W by adding n words where n is the set of words in Table 1\n",
        "#of RG65 that were not included in the top 5000 words from the Brown corpus. Denote the\n",
        "#total number of words in W as |W|.\n",
        "\n",
        "w_freq = nltk.FreqDist(word.lower() for word in brown.words())\n",
        "w = [word[0] for word in w_freq.most_common(5000)]\n",
        "\n",
        "top_5 = w[:5]\n",
        "bottom_5 = w[-5:]\n",
        "\n",
        "print(\"Top 5 common words: {0}\".format(str(top_5)))\n",
        "print(\"Bottom 5 common words: {0}\".format(str(bottom_5)))\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/RG65.csv\", newline=\"\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    rg65 = [(row[0].split()[1].split('_')[0], row[1].split()[1].split('_')[0]) for row in reader]\n",
        "\n",
        "for pair in rg65:\n",
        "  if pair[0] not in w: w.append(pair[0])\n",
        "  if pair[1] not in w: w.append(pair[1])\n",
        "\n",
        "n = len(w)\n",
        "print(w[:5])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 5 common words: ['the', ',', '.', 'of', 'and']\n",
            "Bottom 5 common words: ['cheek', 'awake', 'pursue', 'peered', 'crawled']\n",
            "['the', ',', '.', 'of', 'and']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D_g14bqZ96p"
      },
      "source": [
        "with open('/content/gdrive/My Drive/brown-RG65.csv', 'w') as f: \n",
        "    # using csv.writer method from CSV package \n",
        "    write = csv.writer(f) \n",
        "    write.writerow(w) "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whi0AcozuRGO"
      },
      "source": [
        "#Step 3. Construct a word-context vector model (denoted by M1) by collecting bigram counts\n",
        "#for words in W. The output should be a |W|Ã—|W| matrix (consider using sparse matrices\n",
        "#for better efficiency), where each row is a word in W, and each column is a context in W\n",
        "#that precedes row words in sentences. For example, if the phrase taxi driver appears 5 times\n",
        "#in the entire corpus, then row taxi and column driver should have a value of 5 in the matrix.\n",
        "\n",
        "m1_freq = nltk.ConditionalFreqDist(nltk.bigrams(word.lower() for word in brown.words()))\n",
        "bigram_probdist = nltk.ConditionalProbDist(m1_freq, nltk.DictionaryProbDist)\n",
        "\n",
        "m1 = np.zeros((n,n))\n",
        "\n",
        "for i, word1 in enumerate(w):\n",
        "    for j, word2 in enumerate(w):\n",
        "        m1[i][j] = m1_freq[word1][word2]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7spRDdBhyaJF"
      },
      "source": [
        "m1_plus = np.zeros((n,n))\n",
        "for i, word1 in enumerate(w):\n",
        "  for j, word2 in enumerate(w):\n",
        "    word1_prob = w_freq.freq(word1)\n",
        "    word2_prob = w_freq.freq(word2)\n",
        "    if word1_prob == 0 or word2_prob == 0 : m1_plus[i][j] = 0\n",
        "    else: m1_plus[i][j] = bigram_probdist[word1].prob(word2) / ( word1_prob* word2_prob)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8J4JRxuA-UY"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca10 = PCA(n_components=10)\n",
        "pca100 = PCA(n_components=100)\n",
        "pca300 = PCA(n_components=300)\n",
        "\n",
        "embedding10 = pca10.fit_transform(m1_plus)\n",
        "embedding100 = pca100.fit_transform(m1_plus)\n",
        "embedding300 = pca300.fit_transform(m1_plus)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0r--bSHhH3i",
        "outputId": "047d053a-2c35-4de4-9c82-dd373e3c4798"
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/RG65.csv\", newline=\"\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    rg65 = [(row[0].split()[1].split('_')[0], row[1].split()[1].split('_')[0], row[2]) for row in reader]\n",
        "\n",
        "human_similarity = [float(row[2]) for row in rg65]\n",
        "\n",
        "cos_similarities = []\n",
        "for i in range(len(rg65)):\n",
        "  word1 = rg65[i][0]\n",
        "  word2 = rg65[i][1]\n",
        "  idx_word1 = w.index(word1)\n",
        "  idx_word2 = w.index(word2)\n",
        "  word1_lsavec = embedding300[idx_word1]\n",
        "  word2_lsavec = embedding300[idx_word2]\n",
        "\n",
        "  similarity = 1 - distance.cdist([word1_lsavec], [word2_lsavec], metric='cosine')[0][0]\n",
        "  cos_similarities.append(similarity)\n",
        "\n",
        "lst = [(rg65[i][0], rg65[i][1], rg65[i][2], cos_similarities[i]) for i in range(len(rg65))]\n",
        "df = pd.DataFrame(lst, columns=['word1', 'word2', 'human similarity', 'cosine similarity'])\n",
        "\n",
        "print(df)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         word1      word2 human similarity  cosine similarity\n",
            "0         cord      smile             0.02          -0.006084\n",
            "1      rooster     voyage             0.04           0.919071\n",
            "2         noon     string             0.04           0.001595\n",
            "3        fruit    furnace             0.05          -0.002557\n",
            "4    autograph      shore             0.06           0.149784\n",
            "..         ...        ...              ...                ...\n",
            "60     cushion     pillow             3.84          -0.047482\n",
            "61    cemetery  graveyard             3.88          -0.032566\n",
            "62  automobile        car             3.92           0.006757\n",
            "63      midday       noon             3.94           0.093577\n",
            "64         gem      jewel             3.94           0.999739\n",
            "\n",
            "[65 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ2gLmAWmOfP",
        "outputId": "d1626846-9703-44cc-f833-1b9a145adea2"
      },
      "source": [
        "# Pearson correlation between LSA vectors and human judgement of similarities. \n",
        "\n",
        "pearsonr = stats.pearsonr(cos_similarities, human_similarity)\n",
        "print(\"Pearson Correlation of LSA is : {}\".format(pearsonr))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pearson Correlation of LSA is : (0.14298801225562885, 0.2558358862360781)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_YbRPHpjTjD",
        "outputId": "a94c328f-02a1-44bd-b422-f0d79ebf623a"
      },
      "source": [
        "# Semantic and Syntactic analogy\n",
        "analogy_file = '/content/gdrive/My Drive/word-test.v1.txt'\n",
        "with open(analogy_file) as f:\n",
        "    content = f.readlines()\n",
        "\n",
        "analogy_pairs = []\n",
        "semantic_pairs = []\n",
        "syntactic_pairs = []\n",
        "\n",
        "syntactic = False\n",
        "for line in content:\n",
        "  items = line.split()\n",
        "  items = [item.lower() for item in items]\n",
        "  if items[0] == \"amazing\": syntactic = True # start of syntactic pairs\n",
        "  result = all(elem in w  for elem in items)\n",
        "  if result and not syntactic: semantic_pairs.append(items)\n",
        "  if result and syntactic: syntactic_pairs.append(items)\n",
        "\n",
        "print(\"Total semantic pairs: \", len(semantic_pairs))\n",
        "print(\"Total syntactic pairs: \", len(syntactic_pairs))\n",
        "\n",
        "semantic_pairs = [pair for pair in semantic_pairs if len(pair) == 4]\n",
        "syntactic_pairs = [pair for pair in syntactic_pairs if len(pair) == 4]\n",
        "\n",
        "def get_accuracy(analogy_pairs):\n",
        "  n = len(analogy_pairs)\n",
        "\n",
        "  word1_vec = [embedding300[w.index(pair[0])] for pair in analogy_pairs]\n",
        "  word2_vec = [embedding300[w.index(pair[1])] for pair in analogy_pairs]\n",
        "  word3_vec = [embedding300[w.index(pair[2])] for pair in analogy_pairs]\n",
        "\n",
        "  predict_vecs = [word2_vec[i] - word1_vec[i] + word3_vec[i] for i in range(n)]\n",
        "\n",
        "  def closest_node(node, nodes):\n",
        "      closest_index = distance.cdist([node], nodes, metric='cosine').argsort()\n",
        "      return closest_index[0][1]\n",
        "\n",
        "  count = 0\n",
        "  for i in range(n):\n",
        "    c = closest_node(predict_vecs[i], embedding300)\n",
        "    if analogy_pairs[i][3] == w[c]: count = count + 1\n",
        "  \n",
        "  return count / n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total semantic pairs:  163\n",
            "Total syntactic pairs:  2045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9I7vk6myPwd",
        "outputId": "33de8036-c40d-44dc-84b7-a5972b0a0574"
      },
      "source": [
        "semantic_accuracy = get_accuracy(semantic_pairs)\n",
        "syntactic_accuracy = get_accuracy(syntactic_pairs)\n",
        "\n",
        "print(\"Semantic Accuracy of LSA: \", semantic_accuracy)\n",
        "print(\"Syntactic Accuracy of LSA: \", syntactic_accuracy)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Semantic Accuracy of LSA:  0.018518518518518517\n",
            "Syntactic Accuracy of LSA:  0.006356968215158924\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}